import os
from dotenv import load_dotenv
import streamlit as st
from openai import OpenAI

load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
client = OpenAI(api_key=OPENAI_API_KEY)


def get_best_model(user_prompt) -> str:
    router_prompt = f"""
    ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•´ì„œ ì•„ë˜ ê·œì¹™ì— ë”°ë¼ ê°€ì¥ ì í•©í•œ ëª¨ë¸ì„ í•˜ë‚˜ë§Œ ì„ íƒí•˜ì„¸ìš”.

    [ì„ íƒ ê¸°ì¤€]
    - gpt-5-nano: ì´ˆê³ ì†Â·ì§§ì€ ì‘ë‹µì´ ê°€ì¥ ì¤‘ìš”í•œ ê²½ìš°
    - gpt-4.1: ì‚¬ìš©ì ì§ˆë¬¸ì´ 1,000ì ì´ìƒ ê¸´ í…ìŠ¤íŠ¸ì˜ ë²ˆì—­/ìš”ì•½ ì‘ì—…ì¸ ê²½ìš° 
    - gpt-4o: ì¼ë°˜ì ì¸ ì§ˆë¬¸Â·ëŒ€í™” ë“± ì¼ìƒì  ì‘ì—…(ê¸°ë³¸ê°’)
    - gpt-5-thinking: ì½”ë”©/ë””ë²„ê¹…/ì•Œê³ ë¦¬ì¦˜/ì‹œìŠ¤í…œ ì„¤ê³„, ìˆ˜í•™Â·ë…¼ë¦¬ ë¬¸ì œ, ì œì•½ ë§ì€ ê³„íš ìˆ˜ë¦½ ë“± ë³µì¡í•œ ë‹¤ë‹¨ê³„ ì¶”ë¡ ì´ í•„ìš”í•œ ê²½ìš°
      ë‹¤ìŒ í‘œí˜„ì´ í•˜ë‚˜ë¼ë„ í¬í•¨ë˜ë©´ ë°˜ë“œì‹œ gpt-5-thinking ì„ íƒ
      "ê¹Šê²Œ ìƒê°", "ì²œì²œíˆ ìƒê°", "ì‹¬ì‚¬ìˆ™ê³ ", "ë‹¨ê³„ë³„ë¡œ", "ë…¼ë¦¬ì ìœ¼ë¡œ", "ë©´ë°€íˆ", "ì¶”ë¡ í•´", "ì‚¬ê³  ê³¼ì •ì„", "ìì„¸íˆ ë¶„ì„"


    ì¶œë ¥ í˜•ì‹: ì•„ë˜ ì¤‘ ì •í™•íˆ í•˜ë‚˜ë§Œ ì¶œë ¥í•˜ì„¸ìš”
    gpt-5-nano | gpt-4o | gpt-5-thinking | gpt-4.1
    
    
    <ì‚¬ìš©ì ì§ˆë¬¸/>
    {user_prompt}
    </ì‚¬ìš©ì ì§ˆë¬¸>

    """

    response = client.responses.create(
        model="gpt-5-nano",
        input=router_prompt,
        reasoning={"effort": "minimal"},
    )
    
    recommended_model = response.output_text
    return recommended_model



def llm_stream_events(model: str, messages: list):
    if model == "gpt-5-thinking":
        stream = client.responses.stream(
            model="gpt-5",
            input=messages,
            reasoning={"effort": "medium", "summary": "detailed"},
        )
    elif model == "gpt-5-nano":
        stream = client.responses.stream(
            model=model,
            input=messages,
            reasoning={"effort": "minimal"},
        )
    else:
        stream = client.responses.stream(
            model=model,
            input=messages,
        )
    return stream

def main():

    if "openai_model" not in st.session_state:
        st.session_state["openai_model"] = "ìë™"
    if "messages" not in st.session_state:
        st.session_state.messages = []  
    MODEL_OPTIONS = [
        "ìë™",
        "gpt-4o",
        "gpt-5-nano",
        "gpt-5-thinking",
        "gpt-4.1",
    ]


    left_col, right_col = st.columns([2, 1])
    with left_col:
        st.write("## ë¼ìš°íŒ… ì—ì´ì „íŠ¸ ")
    with right_col:
        selected_model = st.selectbox(
            "ëª¨ë¸ ì„ íƒ",
            MODEL_OPTIONS,
            index=MODEL_OPTIONS.index(st.session_state["openai_model"])
            if st.session_state["openai_model"] in MODEL_OPTIONS else 0,
            help="ëŒ€í™”ì— ì‚¬ìš©í•  ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”.",
        )
    st.session_state["openai_model"] = selected_model


    for message in st.session_state.messages:
        icon = ":material/network_intelligence:" if message["role"] == "assistant" else ":material/person:"
        with st.chat_message(message["role"], avatar=icon):
            st.markdown(message["content"])

    prompt = st.chat_input("ì—ì´ì „íŠ¸ì—ê²Œ ë¬¼ì–´ë³´ê¸°")


    if prompt:
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user", avatar=":material/person:"):
            st.markdown(prompt)

        with st.chat_message("assistant", avatar=":material/network_intelligence:"):
            answer_ph = st.empty()
            if st.session_state["openai_model"] == "ìë™":
                with st.spinner("ğŸ¤– ëª¨ë¸ ì„ íƒì¤‘"):
                    selected_model = get_best_model(prompt)
                st.toast(f"ğŸ¯ ëª¨ë¸ ì„ íƒ: **{selected_model}**")
            else:
                selected_model = st.session_state["openai_model"]
           
            if selected_model == "gpt-5-thinking":
                with st.expander("ğŸ” ì¶”ë¡  ê³¼ì •", expanded=False):
                    reasoning_ph = st.empty()
            else:
                reasoning_ph = None

            answer_chunks = []
            reasoning_chunks = []
            stream = llm_stream_events(
                model=selected_model,
                messages=st.session_state.messages,
            )

            with stream as s:
                for event in s:
                    if event.type == "response.reasoning_summary_text.delta":
                        reasoning_chunks.append(event.delta)
                        reasoning_ph.markdown("".join(reasoning_chunks))
                    elif event.type == "response.output_text.delta":
                        answer_chunks.append(event.delta)
                        answer_ph.markdown("".join(answer_chunks))
                s.until_done()
            response_text = "".join(answer_chunks)

        st.session_state.messages.append({"role": "assistant", "content": response_text})


if __name__ == "__main__":
    main()
